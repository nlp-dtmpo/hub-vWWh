# _*_ coding : utf-8 _*_
# @Time : 2026/1/29 08:38
# @Author : MR.江
# @File : GRU实现文分分类
# @Project : nlp20
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from torch.utils.data import Dataset, DataLoader

# 加载数据
dataset = pd.read_csv("dataset.csv", sep='\t', header=None)
texts = dataset[0].tolist()
string_label = dataset[1].tolist()
label_to_index = {label: i for i, label in enumerate(set(string_label))}
num_label = [label_to_index[label] for label in string_label]

char_to_index = {'<pad>': 0}
for text in texts:
    for char in text:
        if char not in char_to_index:
            char_to_index[char] = len(char_to_index)
index_to_char = {i: char for char, i in char_to_index.items()}
vocab_size = len(char_to_index)
max_len = 40


class CharGRUDataset(Dataset):
    def __init__(self, texts, labels, char_to_index, max_len):
        self.texts = texts
        self.labels = torch.tensor(labels, dtype=torch.long)
        self.char_to_index = char_to_index
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        indice = [self.char_to_index.get(char, 0) for char in text[:self.max_len]]
        indice += [0] * (self.max_len - len(indice))
        return torch.tensor(indice, dtype=torch.long), self.labels[idx]


class GRUClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layer=2, dropout=0.3):
        super(GRUClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layer, batch_first=True, dropout=dropout,
                          bidirectional=True)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, hidden = self.gru(embedded)
        last_hidden = hidden[-1]
        dropped = self.dropout(last_hidden)
        out = self.fc(dropped)
        return out


rnn_dataset = CharGRUDataset(texts, num_label, char_to_index, max_len)
dataloader = DataLoader(rnn_dataset, batch_size=32, shuffle=True)
embedding_dim = 64
hidden_dim = 128
output_dim = len(label_to_index)
model = GRUClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
epoch_num = 10
for epoch in range(epoch_num):
    model.train()
    for idx, (input, label) in enumerate(dataloader):
        optimizer.zero_grad()
        output = model(input)
        loss = criterion(output, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 梯度裁剪，防止RNN梯度爆炸
        optimizer.step()
        if idx % 50 == 0:
            print(f"batch个数 {idx}, 当前batch_loss:{loss.item()}")
    scheduler.step()  # 学习率自动调整


def classifier_text(text, model, char_to_index, max_len, index_to_label):
    tokenized = [char_to_index.get(char, 0) for char in text[: max_len]]
    tokenized += [0] * (max_len - len(tokenized))
    input_tensor = torch.tensor(tokenized, dtype=torch.long).unsqueeze(0)
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)
    _, pre_index = torch.max(output, 1)
    pre_index = pre_index.item()
    pre_label = index_to_label[pre_index]
    return pre_label


index_to_label = {i: label for label, i in label_to_index.items()}
new_text = "帮我导航到北京"
pre_class = classifier_text(new_text, model, char_to_index, max_len, index_to_label)
print(f" 输入：{new_text}, 预测：{pre_class}")
