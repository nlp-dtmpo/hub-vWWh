本方案是针对客服工作台FAQ智能问答场景，仅基于Sentence-BERT孪生网络模型和常规存储组件（Redis/MySQL），实现用户问题与FAQ库的语义相似度匹配。方案覆盖模型训练、离线编码、在线检索三大环节，适用于FAQ变体数量≤5万条的中小型客服系统。
核心任务：通过对比学习微调Sentence-BERT，使其在客服场景下具备语义相似度判别能力。
训练数据构造：从历史客服会话日志中提取用户实际提问，将用户问题与客服最终点击的FAQ标准问作为正样本对，与随机抽取的其他FAQ标准问作为负样本对，正负样本比例控制在1:3至1:5之间。
损失函数选择：采用Multiple Negatives Ranking Loss。该损失函数的数学本质是让正确配对的相似度远高于同一批次内的所有错误配对，通过Softmax函数将相似度分数转化为概率，最大化正确配对的概率。这与客服问答场景天然契合——用户问题只有一个正确答案，其余均为错误答案。
输出产物：一个针对客服场景优化后的Sentence-BERT模型文件（pytorch_model.bin），该模型将贯穿离线编码与在线编码全流程，确保环境一致性。

核心任务：将FAQ库中的所有标准问及相似问，通过已微调的SBERT模型转化为768维（或384维）语义向量，并存储至可快速读取的存储介质。
文本预处理流水线：与模型训练阶段保持完全一致的操作序列

向量编码执行：采用Sentence-Transformers框架的encode方法，设置normalize_embeddings=True参数。该参数使得输出向量的L2模长为1，即单位向量。单位化的核心价值在于：两个单位向量的点积直接等于其余弦相似度，无需再进行除法运算，在线阶段可节省约30%的计算开销。

向量存储方案：采用Redis Hash结构。
