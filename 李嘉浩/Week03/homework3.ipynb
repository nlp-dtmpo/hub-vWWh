{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccabae24-65a8-4a16-bdc8-fe40e5a68e32",
   "metadata": {},
   "source": [
    "# 作业problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca0f04-c45f-4166-a325-881d88ed4d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset = pd.read_csv(\"../Week01/dataset.csv\", sep=\"\\t\", header=None)\n",
    "texts = dataset[0].tolist()\n",
    "string_labels = dataset[1].tolist()\n",
    "\n",
    "label_to_index = {label: i for i, label in enumerate(set(string_labels))}\n",
    "numerical_labels = [label_to_index[label] for label in string_labels]\n",
    "\n",
    "char_to_index = {'<pad>': 0}\n",
    "for text in texts:\n",
    "    for char in text:\n",
    "        if char not in char_to_index:\n",
    "            char_to_index[char] = len(char_to_index)\n",
    "\n",
    "index_to_char = {i: char for char, i in char_to_index.items()}\n",
    "vocab_size = len(char_to_index)\n",
    "\n",
    "# max length 最大输入的文本长度\n",
    "max_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903fc4fa-aa6c-4357-aebf-ef1b4e66ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集 - 》 为每个任务定义单独的数据集的读取方式，这个任务的输入和输出\n",
    "# 统一的写法，底层pytorch 深度学习 / 大模型\n",
    "class CharLSTMDataset(Dataset):\n",
    "    # 初始化\n",
    "    def __init__(self, texts, labels, char_to_index, max_len):\n",
    "        self.texts = texts # 文本输入\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long) # 文本对应的标签\n",
    "        self.char_to_index = char_to_index # 字符到索引的映射关系\n",
    "        self.max_len = max_len # 文本最大输入长度\n",
    "\n",
    "    # 返回数据集样本个数\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # 获取当个样本\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        # pad and crop\n",
    "        indices = [self.char_to_index.get(char, 0) for char in text[:self.max_len]]\n",
    "        indices += [0] * (self.max_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long), self.labels[idx]\n",
    "\n",
    "# a = CharLSTMDataset()\n",
    "# len(a) -> a.__len__\n",
    "# a[0] -> a.__getitem__\n",
    "\n",
    "\n",
    "# --- NEW LSTM Model Class ---\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # 词表大小 转换后维度的维度\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # 随机编码的过程， 可训练的\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # 循环层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size * seq length -》 batch size * seq length * embedding_dim\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # batch size * seq length * embedding_dim -》 batch size * seq length * hidden_dim\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(embedded)\n",
    "\n",
    "        # batch size * output_dim\n",
    "        out = self.fc(hidden_state.squeeze(0))\n",
    "        return out\n",
    "    \n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        # 词表大小 转换后维度的维度\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # 随机编码的过程， 可训练的\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # 循环层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size * seq length -》 batch size * seq length * embedding_dim\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # batch size * seq length * embedding_dim -》 batch size * seq length * hidden_dim\n",
    "        lstm_out ,hidden_state = self.rnn(embedded)\n",
    "\n",
    "        # batch size * output_dim\n",
    "        out = self.fc(hidden_state.squeeze(0))\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "\n",
    "        # 词表大小 转换后维度的维度\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # 随机编码的过程， 可训练的\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)  # 循环层\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch size * seq length -》 batch size * seq length * embedding_dim\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # batch size * seq length * embedding_dim -》 batch size * seq length * hidden_dim\n",
    "        lstm_out ,hidden_state = self.gru(embedded)\n",
    "\n",
    "        # batch size * output_dim\n",
    "        out = self.fc(hidden_state.squeeze(0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c833ad0-ac25-480c-8b78-cfd9e9f0602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 个数 0, 当前Batch Loss: 2.510615110397339\n",
      "Batch 个数 50, 当前Batch Loss: 2.270514488220215\n",
      "Batch 个数 100, 当前Batch Loss: 2.3900198936462402\n",
      "Batch 个数 150, 当前Batch Loss: 2.275806188583374\n",
      "Batch 个数 200, 当前Batch Loss: 2.38065242767334\n",
      "Batch 个数 250, 当前Batch Loss: 2.436516046524048\n",
      "Batch 个数 300, 当前Batch Loss: 2.412123441696167\n",
      "Batch 个数 350, 当前Batch Loss: 2.4185950756073\n",
      "Epoch [1/4], Loss: 2.3603\n",
      "Batch 个数 0, 当前Batch Loss: 2.4016170501708984\n",
      "Batch 个数 50, 当前Batch Loss: 2.3760714530944824\n",
      "Batch 个数 100, 当前Batch Loss: 2.230572462081909\n",
      "Batch 个数 150, 当前Batch Loss: 1.7764116525650024\n",
      "Batch 个数 200, 当前Batch Loss: 1.7029638290405273\n",
      "Batch 个数 250, 当前Batch Loss: 1.8981480598449707\n",
      "Batch 个数 300, 当前Batch Loss: 1.4930039644241333\n",
      "Batch 个数 350, 当前Batch Loss: 1.346110224723816\n",
      "Epoch [2/4], Loss: 1.8994\n",
      "Batch 个数 0, 当前Batch Loss: 1.4896399974822998\n",
      "Batch 个数 50, 当前Batch Loss: 1.2709661722183228\n",
      "Batch 个数 100, 当前Batch Loss: 1.1809909343719482\n",
      "Batch 个数 150, 当前Batch Loss: 1.3678020238876343\n",
      "Batch 个数 200, 当前Batch Loss: 1.2827773094177246\n",
      "Batch 个数 250, 当前Batch Loss: 0.8924839496612549\n",
      "Batch 个数 300, 当前Batch Loss: 0.7977774739265442\n",
      "Batch 个数 350, 当前Batch Loss: 1.0116097927093506\n",
      "Epoch [3/4], Loss: 1.1984\n",
      "Batch 个数 0, 当前Batch Loss: 0.9463824033737183\n",
      "Batch 个数 50, 当前Batch Loss: 0.7141762971878052\n",
      "Batch 个数 100, 当前Batch Loss: 1.065899133682251\n",
      "Batch 个数 150, 当前Batch Loss: 0.534665584564209\n",
      "Batch 个数 200, 当前Batch Loss: 0.5912585258483887\n",
      "Batch 个数 250, 当前Batch Loss: 0.40140822529792786\n",
      "Batch 个数 300, 当前Batch Loss: 0.5891070365905762\n",
      "Batch 个数 350, 当前Batch Loss: 0.2633271813392639\n",
      "Epoch [4/4], Loss: 0.6407\n",
      "输入 '帮我导航到北京' 预测为: 'Weather-Query'\n",
      "输入 '查询明天北京的天气' 预测为: 'Weather-Query'\n"
     ]
    }
   ],
   "source": [
    "# --- Training and Prediction ---\n",
    "lstm_dataset = CharLSTMDataset(texts, numerical_labels, char_to_index, max_len)\n",
    "dataloader = DataLoader(lstm_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_to_index)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Batch 个数 {idx}, 当前Batch Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "def classify_text_lstm(text, model, char_to_index, max_len, index_to_label):\n",
    "    indices = [char_to_index.get(char, 0) for char in text[:max_len]]\n",
    "    indices += [0] * (max_len - len(indices))\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    _, predicted_index = torch.max(output, 1)\n",
    "    predicted_index = predicted_index.item()\n",
    "    predicted_label = index_to_label[predicted_index]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "index_to_label = {i: label for label, i in label_to_index.items()}\n",
    "\n",
    "new_text = \"帮我导航到北京\"\n",
    "predicted_class = classify_text_lstm(new_text, model, char_to_index, max_len, index_to_label)\n",
    "print(f\"输入 '{new_text}' 预测为: '{predicted_class}'\")\n",
    "\n",
    "new_text_2 = \"查询明天北京的天气\"\n",
    "predicted_class_2 = classify_text_lstm(new_text_2, model, char_to_index, max_len, index_to_label)\n",
    "print(f\"输入 '{new_text_2}' 预测为: '{predicted_class_2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a319822e-faf5-4284-8876-1347e3021247",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 个数 0, 当前Batch Loss: 2.43194580078125\n",
      "Batch 个数 50, 当前Batch Loss: 2.4570977687835693\n",
      "Batch 个数 100, 当前Batch Loss: 2.3653411865234375\n",
      "Batch 个数 150, 当前Batch Loss: 2.313234806060791\n",
      "Batch 个数 200, 当前Batch Loss: 2.3763866424560547\n",
      "Batch 个数 250, 当前Batch Loss: 2.311861276626587\n",
      "Batch 个数 300, 当前Batch Loss: 2.386493444442749\n",
      "Batch 个数 350, 当前Batch Loss: 2.338043689727783\n",
      "Epoch [1/4], Loss: 2.3681\n",
      "Batch 个数 0, 当前Batch Loss: 2.3902225494384766\n",
      "Batch 个数 50, 当前Batch Loss: 2.270555019378662\n",
      "Batch 个数 100, 当前Batch Loss: 2.314812660217285\n",
      "Batch 个数 150, 当前Batch Loss: 2.281960964202881\n",
      "Batch 个数 200, 当前Batch Loss: 2.2433958053588867\n",
      "Batch 个数 250, 当前Batch Loss: 2.36765456199646\n",
      "Batch 个数 300, 当前Batch Loss: 2.425328493118286\n",
      "Batch 个数 350, 当前Batch Loss: 2.4008092880249023\n",
      "Epoch [2/4], Loss: 2.3616\n",
      "Batch 个数 0, 当前Batch Loss: 2.285299062728882\n",
      "Batch 个数 50, 当前Batch Loss: 2.5932559967041016\n",
      "Batch 个数 100, 当前Batch Loss: 2.489384174346924\n",
      "Batch 个数 150, 当前Batch Loss: 2.328425168991089\n",
      "Batch 个数 200, 当前Batch Loss: 2.4043352603912354\n",
      "Batch 个数 250, 当前Batch Loss: 2.332746982574463\n",
      "Batch 个数 300, 当前Batch Loss: 2.3916425704956055\n",
      "Batch 个数 350, 当前Batch Loss: 2.3238964080810547\n",
      "Epoch [3/4], Loss: 2.3670\n",
      "Batch 个数 0, 当前Batch Loss: 2.608604669570923\n",
      "Batch 个数 50, 当前Batch Loss: 2.1712217330932617\n",
      "Batch 个数 100, 当前Batch Loss: 2.2117810249328613\n",
      "Batch 个数 150, 当前Batch Loss: 2.1071219444274902\n",
      "Batch 个数 200, 当前Batch Loss: 2.15925669670105\n",
      "Batch 个数 250, 当前Batch Loss: 2.4479267597198486\n",
      "Batch 个数 300, 当前Batch Loss: 2.117161989212036\n",
      "Batch 个数 350, 当前Batch Loss: 2.163442611694336\n",
      "Epoch [4/4], Loss: 2.2948\n",
      "输入 '帮我导航到北京' 预测为: 'FilmTele-Play'\n",
      "输入 '查询明天北京的天气' 预测为: 'Weather-Query'\n"
     ]
    }
   ],
   "source": [
    "# --- RNN ---\n",
    "lstm_dataset = CharLSTMDataset(texts, numerical_labels, char_to_index, max_len)\n",
    "dataloader = DataLoader(lstm_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_to_index)\n",
    "\n",
    "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Batch 个数 {idx}, 当前Batch Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "def classify_text_lstm(text, model, char_to_index, max_len, index_to_label):\n",
    "    indices = [char_to_index.get(char, 0) for char in text[:max_len]]\n",
    "    indices += [0] * (max_len - len(indices))\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    _, predicted_index = torch.max(output, 1)\n",
    "    predicted_index = predicted_index.item()\n",
    "    predicted_label = index_to_label[predicted_index]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "index_to_label = {i: label for label, i in label_to_index.items()}\n",
    "\n",
    "new_text = \"帮我导航到北京\"\n",
    "predicted_class = classify_text_lstm(new_text, model, char_to_index, max_len, index_to_label)\n",
    "print(f\"输入 '{new_text}' 预测为: '{predicted_class}'\")\n",
    "\n",
    "new_text_2 = \"查询明天北京的天气\"\n",
    "predicted_class_2 = classify_text_lstm(new_text_2, model, char_to_index, max_len, index_to_label)\n",
    "print(f\"输入 '{new_text_2}' 预测为: '{predicted_class_2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2057f481-6b80-4641-a3a1-4885fed5bd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 个数 0, 当前Batch Loss: 2.458599090576172\n",
      "Batch 个数 50, 当前Batch Loss: 2.518076181411743\n",
      "Batch 个数 100, 当前Batch Loss: 2.2380869388580322\n",
      "Batch 个数 150, 当前Batch Loss: 1.2760915756225586\n",
      "Batch 个数 200, 当前Batch Loss: 1.3909823894500732\n",
      "Batch 个数 250, 当前Batch Loss: 0.9436416029930115\n",
      "Batch 个数 300, 当前Batch Loss: 0.7190680503845215\n",
      "Batch 个数 350, 当前Batch Loss: 0.7907894849777222\n",
      "Epoch [1/4], Loss: 1.3507\n",
      "Batch 个数 0, 当前Batch Loss: 0.39854857325553894\n",
      "Batch 个数 50, 当前Batch Loss: 0.3317549228668213\n",
      "Batch 个数 100, 当前Batch Loss: 0.300327867269516\n",
      "Batch 个数 150, 当前Batch Loss: 0.20964555442333221\n",
      "Batch 个数 200, 当前Batch Loss: 0.2956169843673706\n",
      "Batch 个数 250, 当前Batch Loss: 0.4843326210975647\n",
      "Batch 个数 300, 当前Batch Loss: 0.6049566864967346\n",
      "Batch 个数 350, 当前Batch Loss: 0.7831342816352844\n",
      "Epoch [2/4], Loss: 0.4439\n",
      "Batch 个数 0, 当前Batch Loss: 0.3295494616031647\n",
      "Batch 个数 50, 当前Batch Loss: 0.40670162439346313\n",
      "Batch 个数 100, 当前Batch Loss: 0.20665298402309418\n",
      "Batch 个数 150, 当前Batch Loss: 0.1732630878686905\n",
      "Batch 个数 200, 当前Batch Loss: 0.2910917401313782\n",
      "Batch 个数 250, 当前Batch Loss: 0.28409543633461\n",
      "Batch 个数 300, 当前Batch Loss: 0.27072760462760925\n",
      "Batch 个数 350, 当前Batch Loss: 0.7396062016487122\n",
      "Epoch [3/4], Loss: 0.3006\n",
      "Batch 个数 0, 当前Batch Loss: 0.1988549381494522\n",
      "Batch 个数 50, 当前Batch Loss: 0.1126062348484993\n",
      "Batch 个数 100, 当前Batch Loss: 0.2385307103395462\n",
      "Batch 个数 150, 当前Batch Loss: 0.22076484560966492\n",
      "Batch 个数 200, 当前Batch Loss: 0.4479662775993347\n",
      "Batch 个数 250, 当前Batch Loss: 0.4796810746192932\n",
      "Batch 个数 300, 当前Batch Loss: 0.12152310460805893\n",
      "Batch 个数 350, 当前Batch Loss: 0.22446176409721375\n",
      "Epoch [4/4], Loss: 0.2169\n",
      "输入 '帮我导航到北京' 预测为: 'Travel-Query'\n",
      "输入 '查询明天北京的天气' 预测为: 'Weather-Query'\n"
     ]
    }
   ],
   "source": [
    "# --- RNN ---\n",
    "lstm_dataset = CharLSTMDataset(texts, numerical_labels, char_to_index, max_len)\n",
    "dataloader = DataLoader(lstm_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_to_index)\n",
    "\n",
    "model = GRUClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for idx, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Batch 个数 {idx}, 当前Batch Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "def classify_text_lstm(text, model, char_to_index, max_len, index_to_label):\n",
    "    indices = [char_to_index.get(char, 0) for char in text[:max_len]]\n",
    "    indices += [0] * (max_len - len(indices))\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    _, predicted_index = torch.max(output, 1)\n",
    "    predicted_index = predicted_index.item()\n",
    "    predicted_label = index_to_label[predicted_index]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "index_to_label = {i: label for label, i in label_to_index.items()}\n",
    "\n",
    "new_text = \"帮我导航到北京\"\n",
    "predicted_class = classify_text_lstm(new_text, model, char_to_index, max_len, index_to_label)\n",
    "print(f\"输入 '{new_text}' 预测为: '{predicted_class}'\")\n",
    "\n",
    "new_text_2 = \"查询明天北京的天气\"\n",
    "predicted_class_2 = classify_text_lstm(new_text_2, model, char_to_index, max_len, index_to_label)\n",
    "print(f\"输入 '{new_text_2}' 预测为: '{predicted_class_2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039533e-f80d-4575-ad5d-af88170d2811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
