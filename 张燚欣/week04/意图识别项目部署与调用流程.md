ğŸ“ é¡¹ç›®ç»“æ„åˆ†æ
text
æ„å›¾è¯†åˆ«é¡¹ç›®/
â”œâ”€â”€ data/                    # æ•°æ®é›†ç›®å½•
â”‚   â”œâ”€â”€ custom_dataset.csv   # è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆæ ¸å¿ƒæ•°æ®ï¼‰
â”‚   â””â”€â”€ label_mapping.json   # æ ‡ç­¾æ˜ å°„æ–‡ä»¶
â”œâ”€â”€ models/                  # è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚   â”œâ”€â”€ best_bert_model/     # BERTæ¨¡å‹æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ config.json      # æ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin  # æ¨¡å‹æƒé‡
â”‚   â”‚   â””â”€â”€ tokenizer_config.json  # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ results/                 # è®­ç»ƒç»“æœ
â”‚   â”œâ”€â”€ classification_report.txt  # åˆ†ç±»æŠ¥å‘Š
â”‚   â”œâ”€â”€ training_history.json     # è®­ç»ƒå†å²
â”‚   â””â”€â”€ training_history.png      # è®­ç»ƒå›¾è¡¨
â”œâ”€â”€ bert_sentiment_finetuning.py  # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ README.md               # é¡¹ç›®è¯´æ˜
â””â”€â”€ requirements.txt        # ä¾èµ–åŒ…
ğŸ”§ ç¬¬ä¸€ç« ï¼šæœ¬åœ°ç¯å¢ƒé…ç½®ï¼ˆStep-by-Stepï¼‰
1.1 ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥
bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆéœ€è¦3.8+ï¼‰
python --version

# æ£€æŸ¥pipç‰ˆæœ¬
pip --version

# æ£€æŸ¥æ˜¯å¦æœ‰GPUï¼ˆå¯é€‰ï¼‰
nvidia-smi  # å¦‚æœæœ‰NVIDIA GPU
1.2 åˆ›å»ºé¡¹ç›®ç›®å½•
bash
# å…‹éš†æˆ–åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir intent_recognition_project
cd intent_recognition_project

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶ï¼ˆæ ¹æ®å›¾ç‰‡ç»“æ„ï¼‰
# ç¡®ä¿æœ‰ä»¥ä¸‹æ–‡ä»¶ï¼š
# - data/custom_dataset.csv
# - data/label_mapping.json  
# - models/best_bert_model/...
# - results/...
# - bert_sentiment_finetuning.py
# - requirements.txt
# - README.md
1.3 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
bash
# æ–¹æ³•1ï¼šä½¿ç”¨venvï¼ˆæ¨èï¼‰
python -m venv venv_intent

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
venv_intent\Scripts\activate
# Linux/Mac
source venv_intent/bin/activate

# æ–¹æ³•2ï¼šä½¿ç”¨conda
conda create -n intent_env python=3.8
conda activate intent_env
1.4 å®‰è£…ä¾èµ–åŒ…
bash
# å®‰è£…requirements.txtä¸­çš„ä¾èµ–
pip install -r requirements.txt

# å¦‚æœrequirements.txtä¸å­˜åœ¨ï¼Œå®‰è£…ä»¥ä¸‹æ ¸å¿ƒåŒ…ï¼š
pip install torch transformers flask flask-cors pandas numpy scikit-learn matplotlib requests
1.5 éªŒè¯ç¯å¢ƒ
python
# åˆ›å»ºéªŒè¯è„šæœ¬ verify_environment.py
import torch
import transformers
import flask
import pandas as pd
import numpy as np
import sys

print("="*50)
print("ç¯å¢ƒéªŒè¯æŠ¥å‘Š")
print("="*50)

# æ£€æŸ¥ç‰ˆæœ¬
print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
print(f"Flaskç‰ˆæœ¬: {flask.__version__}")
print(f"Pandasç‰ˆæœ¬: {pd.__version__}")

# æ£€æŸ¥CUDA
if torch.cuda.is_available():
    print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸  ä½¿ç”¨CPUæ¨¡å¼")

# æ£€æŸ¥å…³é”®æ–‡ä»¶
import os
required_files = [
    "data/custom_dataset.csv",
    "data/label_mapping.json",
    "models/best_bert_model/pytorch_model.bin"
]

for file in required_files:
    if os.path.exists(file):
        print(f"âœ… {file} å­˜åœ¨")
    else:
        print(f"âŒ {file} ç¼ºå¤±")

print("="*50)
print("ç¯å¢ƒéªŒè¯å®Œæˆï¼")
è¿è¡ŒéªŒè¯ï¼š

bash
python verify_environment.py
ğŸš€ ç¬¬äºŒç« ï¼šé¡¹ç›®è¿è¡Œä¸æµ‹è¯•
2.1 æ•°æ®é›†åˆ†æ
é¦–å…ˆæŸ¥çœ‹æ•°æ®æ ¼å¼ï¼š

python
import pandas as pd
import json

# æŸ¥çœ‹æ•°æ®é›†
df = pd.read_csv('data/custom_dataset.csv')
print("æ•°æ®é›†é¢„è§ˆ:")
print(df.head())
print(f"\næ•°æ®é‡: {len(df)}")
print(f"ç±»åˆ«æ•°: {df['label'].nunique()}")

# æŸ¥çœ‹æ ‡ç­¾æ˜ å°„
with open('data/label_mapping.json', 'r', encoding='utf-8') as f:
    label_map = json.load(f)
print("\næ ‡ç­¾æ˜ å°„:")
for label_id, label_name in label_map.items():
    print(f"  {label_id}: {label_name}")
2.2 æ¨¡å‹æµ‹è¯•ï¼ˆå¦‚æœå·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
python
# test_model.py
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import json

# åŠ è½½æ ‡ç­¾æ˜ å°„
with open('data/label_mapping.json', 'r', encoding='utf-8') as f:
    label_map = json.load(f)
    id_to_label = {int(k): v for k, v in label_map.items()}

# åŠ è½½æ¨¡å‹
print("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
model_path = "models/best_bert_model"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path)
model.eval()

# æµ‹è¯•æ ·æœ¬
test_texts = [
    "æˆ‘æƒ³é¢„è®¢æ˜å¤©çš„é…’åº—æˆ¿é—´",
    "è¿™ä¸ªäº§å“æ€ä¹ˆä½¿ç”¨ï¼Ÿ",
    "æˆ‘è¦æŠ•è¯‰æœåŠ¡è´¨é‡",
    "å¸®æˆ‘æŸ¥è¯¢è´¦æˆ·ä½™é¢"
]

print("\næ¨¡å‹æµ‹è¯•ç»“æœ:")
for text in test_texts:
    # ç¼–ç 
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
    
    # é¢„æµ‹
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_id = torch.argmax(logits, dim=1).item()
        
    predicted_label = id_to_label.get(predicted_id, f"æœªçŸ¥ç±»åˆ«{predicted_id}")
    print(f"æ–‡æœ¬: {text}")
    print(f"é¢„æµ‹: {predicted_label} (ID: {predicted_id})")
    print("-" * 40)
è¿è¡Œæµ‹è¯•ï¼š

bash
python test_model.py
ğŸŒ ç¬¬ä¸‰ç« ï¼šAPIæœåŠ¡éƒ¨ç½²
3.1 åˆ›å»ºAPIæœåŠ¡æ–‡ä»¶
api/app.py
python
"""
æ„å›¾è¯†åˆ«APIæœåŠ¡
åŸºäºFlaskæ¡†æ¶
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import json
import os
from transformers import BertTokenizer, BertForSequenceClassification

# åˆå§‹åŒ–Flaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€å˜é‡
model = None
tokenizer = None
label_mapping = {}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def load_model_and_tokenizer():
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    global model, tokenizer, label_mapping
    
    # åŠ è½½æ ‡ç­¾æ˜ å°„
    label_path = "data/label_mapping.json"
    if os.path.exists(label_path):
        with open(label_path, 'r', encoding='utf-8') as f:
            label_mapping = json.load(f)
        print(f"åŠ è½½æ ‡ç­¾æ˜ å°„: {label_mapping}")
    else:
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾æ˜ å°„æ–‡ä»¶ï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„
        label_mapping = {0: "æŸ¥è¯¢", 1: "é¢„è®¢", 2: "æŠ•è¯‰", 3: "å’¨è¯¢", 4: "å…¶ä»–"}
        print(f"ä½¿ç”¨é»˜è®¤æ ‡ç­¾æ˜ å°„: {label_mapping}")
    
    # åŠ è½½æ¨¡å‹
    model_path = "models/best_bert_model"
    if os.path.exists(model_path):
        print(f"åŠ è½½æ¨¡å‹ä»: {model_path}")
        try:
            tokenizer = BertTokenizer.from_pretrained(model_path)
            model = BertForSequenceClassification.from_pretrained(model_path)
            model.to(device)
            model.eval()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    else:
        print("âš ï¸  æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨åœ¨çº¿æ¨¡å‹...")
        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        model = BertForSequenceClassification.from_pretrained(
            'bert-base-chinese',
            num_labels=len(label_mapping)
        )
        model.to(device)
        model.eval()

# å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
@app.before_first_request
def initialize():
    load_model_and_tokenizer()

@app.route('/')
def home():
    """é¦–é¡µ"""
    return jsonify({
        "service": "æ„å›¾è¯†åˆ«API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "/predict": "POST - å•æ¡æ–‡æœ¬é¢„æµ‹",
            "/batch_predict": "POST - æ‰¹é‡é¢„æµ‹",
            "/health": "GET - å¥åº·æ£€æŸ¥",
            "/labels": "GET - è·å–æ ‡ç­¾åˆ—è¡¨"
        }
    })

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    model_status = "loaded" if model else "not loaded"
    return jsonify({
        "status": "healthy",
        "model": model_status,
        "device": str(device)
    })

@app.route('/labels', methods=['GET'])
def get_labels():
    """è·å–æ ‡ç­¾åˆ—è¡¨"""
    return jsonify(label_mapping)

@app.route('/predict', methods=['POST'])
def predict():
    """å•æ¡æ–‡æœ¬é¢„æµ‹"""
    try:
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({
                "error": "ç¼ºå°‘textå‚æ•°",
                "example": {"text": "æˆ‘æƒ³é¢„è®¢é…’åº—"}
            }), 400
        
        text = data['text']
        
        # ç¼–ç æ–‡æœ¬
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            predicted_id = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0][predicted_id].item()
        
        # è·å–æ ‡ç­¾åç§°
        predicted_label = label_mapping.get(str(predicted_id), f"ç±»åˆ«{predicted_id}")
        
        # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        all_probs = {}
        for label_id, label_name in label_mapping.items():
            prob = probabilities[0][int(label_id)].item() if label_id.isdigit() else 0
            all_probs[label_name] = prob
        
        # æ„å»ºå“åº”
        response = {
            "success": True,
            "text": text,
            "prediction": {
                "label": predicted_label,
                "label_id": predicted_id,
                "confidence": confidence
            },
            "probabilities": all_probs,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/batch_predict', methods=['POST'])
def batch_predict():
    """æ‰¹é‡é¢„æµ‹"""
    try:
        data = request.get_json()
        
        if not data or 'texts' not in data:
            return jsonify({
                "error": "ç¼ºå°‘textså‚æ•°",
                "example": {"texts": ["æ–‡æœ¬1", "æ–‡æœ¬2"]}
            }), 400
        
        texts = data['texts']
        
        if not isinstance(texts, list):
            return jsonify({"error": "textså¿…é¡»æ˜¯åˆ—è¡¨"}), 400
        
        results = []
        for text in texts:
            # ä¸ºæ¯æ¡æ–‡æœ¬åˆ›å»ºé¢„æµ‹
            inputs = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=128
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=1)
                predicted_id = torch.argmax(logits, dim=1).item()
                confidence = probabilities[0][predicted_id].item()
            
            predicted_label = label_mapping.get(str(predicted_id), f"ç±»åˆ«{predicted_id}")
            
            results.append({
                "text": text,
                "prediction": predicted_label,
                "confidence": confidence,
                "label_id": predicted_id
            })
        
        return jsonify({
            "success": True,
            "results": results,
            "count": len(results)
        })
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

if __name__ == '__main__':
    # å…ˆåŠ è½½æ¨¡å‹
    load_model_and_tokenizer()
    
    # å¯åŠ¨æœåŠ¡å™¨
    print("="*50)
    print("æ„å›¾è¯†åˆ«APIæœåŠ¡å¯åŠ¨")
    print(f"è®¾å¤‡: {device}")
    print(f"æ ‡ç­¾æ•°é‡: {len(label_mapping)}")
    print("æœåŠ¡åœ°å€: http://localhost:5000")
    print("="*50)
    
    app.run(host='0.0.0.0', port=5000, debug=True)
api/requirements.txt
txt
Flask==2.3.0
flask-cors==4.0.0
torch>=2.0.0
transformers>=4.30.0
pandas>=2.0.0
numpy>=1.24.0
3.2 å¯åŠ¨APIæœåŠ¡
æ–¹æ³•1ï¼šç›´æ¥è¿è¡Œ
bash
# è¿›å…¥apiç›®å½•
cd api

# å®‰è£…APIä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨æœåŠ¡
python app.py

# è¾“å‡ºï¼š
# ==================================================
# æ„å›¾è¯†åˆ«APIæœåŠ¡å¯åŠ¨
# è®¾å¤‡: cpu
# æ ‡ç­¾æ•°é‡: 5
# æœåŠ¡åœ°å€: http://localhost:5000
# ==================================================
#  * Running on all addresses (0.0.0.0)
#  * Running on http://127.0.0.1:5000
æ–¹æ³•2ï¼šä½¿ç”¨Gunicornï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
bash
# å®‰è£…gunicorn
pip install gunicorn

# å¯åŠ¨æœåŠ¡ï¼ˆ4ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰
gunicorn -w 4 -b 0.0.0.0:5000 app:app

# åå°è¿è¡Œ
nohup gunicorn -w 4 -b 0.0.0.0:5000 app:app > api.log 2>&1 &
æ–¹æ³•3ï¼šä½¿ç”¨è„šæœ¬å¯åŠ¨
åˆ›å»ºå¯åŠ¨è„šæœ¬ start_api.shï¼š

bash
#!/bin/bash
echo "å¯åŠ¨æ„å›¾è¯†åˆ«APIæœåŠ¡..."

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv_intent/bin/activate

# è¿›å…¥apiç›®å½•
cd api

# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
PORT=5000
if lsof -Pi :$PORT -sTCP:LISTEN -t >/dev/null ; then
    echo "ç«¯å£ $PORT å·²è¢«å ç”¨ï¼Œæ­£åœ¨åœæ­¢è¿›ç¨‹..."
    fuser -k $PORT/tcp
    sleep 2
fi

# å¯åŠ¨æœåŠ¡
echo "å¯åŠ¨FlaskæœåŠ¡..."
python app.py
èµ‹äºˆæ‰§è¡Œæƒé™ï¼š

bash
chmod +x start_api.sh
./start_api.sh
ğŸ“¡ ç¬¬å››ç« ï¼šAPIè°ƒç”¨ä½“éªŒ
4.1 Pythonå®¢æˆ·ç«¯è°ƒç”¨
clients/python_client.py
python
import requests
import json
import time

class IntentRecognitionClient:
    def __init__(self, base_url="http://localhost:5000"):
        self.base_url = base_url
        self.session = requests.Session()
        
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        url = f"{self.base_url}/health"
        response = self.session.get(url)
        return response.json()
    
    def get_labels(self):
        """è·å–æ ‡ç­¾åˆ—è¡¨"""
        url = f"{self.base_url}/labels"
        response = self.session.get(url)
        return response.json()
    
    def predict(self, text):
        """å•æ¡æ–‡æœ¬é¢„æµ‹"""
        url = f"{self.base_url}/predict"
        payload = {"text": text}
        
        try:
            response = self.session.post(url, json=payload, timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"å“åº”: {response.text}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def batch_predict(self, texts):
        """æ‰¹é‡é¢„æµ‹"""
        url = f"{self.base_url}/batch_predict"
        payload = {"texts": texts}
        
        try:
            response = self.session.post(url, json=payload, timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def test_connection(self):
        """æµ‹è¯•è¿æ¥"""
        print("æµ‹è¯•APIè¿æ¥...")
        
        # å¥åº·æ£€æŸ¥
        health = self.health_check()
        print(f"å¥åº·çŠ¶æ€: {health}")
        
        # è·å–æ ‡ç­¾
        labels = self.get_labels()
        print(f"å¯ç”¨æ ‡ç­¾: {labels}")
        
        return health.get('status') == 'healthy'

# æµ‹è¯•ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = IntentRecognitionClient()
    
    # æµ‹è¯•è¿æ¥
    if client.test_connection():
        print("âœ… APIè¿æ¥æ­£å¸¸")
        
        # æµ‹è¯•å•æ¡é¢„æµ‹
        print("\næµ‹è¯•å•æ¡é¢„æµ‹:")
        test_texts = [
            "æˆ‘æƒ³é¢„è®¢æ˜å¤©çš„é…’åº—",
            "è¿™ä¸ªäº§å“æ€ä¹ˆä½¿ç”¨ï¼Ÿ",
            "æˆ‘è¦æŠ•è¯‰æœåŠ¡æ€åº¦",
            "æŸ¥è¯¢æˆ‘çš„è®¢å•çŠ¶æ€"
        ]
        
        for text in test_texts:
            print(f"\næ–‡æœ¬: {text}")
            result = client.predict(text)
            if result and result.get('success'):
                pred = result['prediction']
                print(f"é¢„æµ‹ç»“æœ: {pred['label']} (ç½®ä¿¡åº¦: {pred['confidence']:.4f})")
            else:
                print("é¢„æµ‹å¤±è´¥")
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æµ‹è¯•æ‰¹é‡é¢„æµ‹
        print("\næµ‹è¯•æ‰¹é‡é¢„æµ‹:")
        batch_result = client.batch_predict(test_texts)
        if batch_result and batch_result.get('success'):
            print(f"æ‰¹é‡å¤„ç† {batch_result['count']} æ¡æ–‡æœ¬:")
            for i, res in enumerate(batch_result['results'], 1):
                print(f"  {i}. {res['text'][:20]}... -> {res['prediction']}")
    else:
        print("âŒ APIè¿æ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
è¿è¡Œå®¢æˆ·ç«¯æµ‹è¯•ï¼š

bash
python clients/python_client.py
4.2 å‘½ä»¤è¡Œè°ƒç”¨ï¼ˆcurlï¼‰
clients/curl_examples.sh
bash
#!/bin/bash
echo "æ„å›¾è¯†åˆ«API curlè°ƒç”¨ç¤ºä¾‹"
echo "=========================="

# 1. å¥åº·æ£€æŸ¥
echo "1. å¥åº·æ£€æŸ¥:"
curl -X GET http://localhost:5000/health
echo -e "\n"

# 2. è·å–æ ‡ç­¾
echo "2. è·å–æ ‡ç­¾åˆ—è¡¨:"
curl -X GET http://localhost:5000/labels
echo -e "\n"

# 3. å•æ¡é¢„æµ‹
echo "3. å•æ¡æ–‡æœ¬é¢„æµ‹:"
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "æˆ‘æƒ³é¢„è®¢æ˜å¤©çš„é…’åº—æˆ¿é—´"}'
echo -e "\n"

# 4. æ‰¹é‡é¢„æµ‹
echo "4. æ‰¹é‡é¢„æµ‹:"
curl -X POST http://localhost:5000/batch_predict \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "é¢„è®¢é¤å…",
      "æŸ¥è¯¢å¤©æ°”",
      "æˆ‘è¦æŠ•è¯‰",
      "å¦‚ä½•ä½¿ç”¨"
    ]
  }'
echo -e "\n"

# 5. å¤æ‚æ–‡æœ¬é¢„æµ‹
echo "5. å¤æ‚æ–‡æœ¬é¢„æµ‹:"
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "ä½ å¥½ï¼Œæˆ‘æƒ³å’¨è¯¢ä¸€ä¸‹å¦‚ä½•é€€è®¢æˆ‘ä¹‹å‰è´­ä¹°çš„æœåŠ¡ï¼Œå¦å¤–è¿˜æƒ³çŸ¥é“é€€æ¬¾æµç¨‹æ˜¯æ€æ ·çš„"}'
è¿è¡Œcurlæµ‹è¯•ï¼š

bash
chmod +x clients/curl_examples.sh
./clients/curl_examples.sh
4.3 Postmanè°ƒç”¨é…ç½®
åˆ›å»ºPostmané›†åˆæ–‡ä»¶ clients/postman_collection.jsonï¼š

json
{
  "info": {
    "name": "æ„å›¾è¯†åˆ«API",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "item": [
    {
      "name": "å¥åº·æ£€æŸ¥",
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "http://localhost:5000/health",
          "protocol": "http",
          "host": ["localhost"],
          "port": "5000",
          "path": ["health"]
        }
      }
    },
    {
      "name": "è·å–æ ‡ç­¾",
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "http://localhost:5000/labels",
          "protocol": "http",
          "host": ["localhost"],
          "port": "5000",
          "path": ["labels"]
        }
      }
    },
    {
      "name": "å•æ¡æ–‡æœ¬é¢„æµ‹",
      "request": {
        "method": "POST",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n  \"text\": \"æˆ‘æƒ³é¢„è®¢æ˜å¤©çš„é…’åº—\"\n}"
        },
        "url": {
          "raw": "http://localhost:5000/predict",
          "protocol": "http",
          "host": ["localhost"],
          "port": "5000",
          "path": ["predict"]
        }
      }
    },
    {
      "name": "æ‰¹é‡é¢„æµ‹",
      "request": {
        "method": "POST",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n  \"texts\": [\n    \"é¢„è®¢é¤å…\",\n    \"æŸ¥è¯¢å¤©æ°”\",\n    \"æˆ‘è¦æŠ•è¯‰\",\n    \"å¦‚ä½•ä½¿ç”¨\"\n  ]\n}"
        },
        "url": {
          "raw": "http://localhost:5000/batch_predict",
          "protocol": "http",
          "host": ["localhost"],
          "port": "5000",
          "path": ["batch_predict"]
        }
      }
    }
  ]
}
å¯¼å…¥åˆ°Postmanåå³å¯ä½¿ç”¨ã€‚

ğŸ³ ç¬¬äº”ç« ï¼šDockerå®¹å™¨åŒ–éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰
5.1 åˆ›å»ºDockerfile
Dockerfile
dockerfile
# ä½¿ç”¨Python 3.8å®˜æ–¹é•œåƒ
FROM python:3.8-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r api/requirements.txt

# ä¸‹è½½BERTæ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
RUN python -c "from transformers import BertTokenizer; BertTokenizer.from_pretrained('bert-base-chinese')"

# æš´éœ²ç«¯å£
EXPOSE 5000

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "api/app.py"]
5.2 Docker Composeé…ç½®
docker-compose.yml
yaml
version: '3.8'

services:
  intent-api:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
    environment:
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
5.3 Dockeréƒ¨ç½²å‘½ä»¤
bash
# æ„å»ºé•œåƒ
docker build -t intent-recognition-api .

# è¿è¡Œå®¹å™¨
docker run -p 5000:5000 -d --name intent-api intent-recognition-api

# æŸ¥çœ‹æ—¥å¿—
docker logs -f intent-api

# åœæ­¢å®¹å™¨
docker stop intent-api

# ä½¿ç”¨docker-compose
docker-compose up -d
docker-compose logs -f
docker-compose down
ğŸ“Š ç¬¬å…­ç« ï¼šæ€§èƒ½æµ‹è¯•ä¸ç›‘æ§
6.1 åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
tests/performance_test.py
python
import requests
import time
import threading
import statistics

class PerformanceTester:
    def __init__(self, base_url="http://localhost:5000"):
        self.base_url = base_url
        
    def single_request_test(self, text, num_requests=100):
        """å•è¯·æ±‚æ€§èƒ½æµ‹è¯•"""
        url = f"{self.base_url}/predict"
        payload = {"text": text}
        
        latencies = []
        successes = 0
        
        print(f"å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼Œ{num_requests}æ¬¡è¯·æ±‚...")
        for i in range(num_requests):
            start_time = time.time()
            
            try:
                response = requests.post(url, json=payload, timeout=5)
                latency = (time.time() - start_time) * 1000  # æ¯«ç§’
                latencies.append(latency)
                
                if response.status_code == 200:
                    successes += 1
                
                if i % 20 == 0:
                    print(f"  å·²å®Œæˆ {i}/{num_requests} æ¬¡è¯·æ±‚")
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥: {e}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if latencies:
            avg_latency = statistics.mean(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            success_rate = successes / num_requests * 100
            
            print(f"\næ€§èƒ½æµ‹è¯•ç»“æœ:")
            print(f"  è¯·æ±‚æ€»æ•°: {num_requests}")
            print(f"  æˆåŠŸæ•°: {successes}")
            print(f"  æˆåŠŸç‡: {success_rate:.2f}%")
            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
            print(f"  æœ€å°å»¶è¿Ÿ: {min_latency:.2f}ms")
            print(f"  æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}ms")
        else:
            print("æ— æœ‰æ•ˆçš„å»¶è¿Ÿæ•°æ®")
    
    def concurrent_test(self, texts, num_threads=10):
        """å¹¶å‘æµ‹è¯•"""
        results = []
        lock = threading.Lock()
        
        def worker(text_id, text):
            url = f"{self.base_url}/predict"
            payload = {"text": text}
            
            start_time = time.time()
            try:
                response = requests.post(url, json=payload, timeout=10)
                latency = (time.time() - start_time) * 1000
                
                with lock:
                    results.append({
                        "text_id": text_id,
                        "success": response.status_code == 200,
                        "latency": latency
                    })
            except Exception as e:
                with lock:
                    results.append({
                        "text_id": text_id,
                        "success": False,
                        "error": str(e)
                    })
        
        print(f"å¼€å§‹å¹¶å‘æµ‹è¯•ï¼Œ{num_threads}ä¸ªçº¿ç¨‹...")
        threads = []
        for i in range(num_threads):
            text = texts[i % len(texts)]
            thread = threading.Thread(target=worker, args=(i, text))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        # åˆ†æç»“æœ
        successes = [r for r in results if r.get('success', False)]
        latencies = [r['latency'] for r in results if 'latency' in r]
        
        if latencies:
            print(f"\nå¹¶å‘æµ‹è¯•ç»“æœ:")
            print(f"  çº¿ç¨‹æ•°: {num_threads}")
            print(f"  æˆåŠŸè¯·æ±‚: {len(successes)}/{len(results)}")
            print(f"  å¹³å‡å»¶è¿Ÿ: {statistics.mean(latencies):.2f}ms")
            print(f"  æ€»å¤„ç†æ—¶é—´: {sum(latencies):.2f}ms")

# è¿è¡Œæ€§èƒ½æµ‹è¯•
if __name__ == "__main__":
    tester = PerformanceTester()
    
    # æµ‹è¯•æ ·æœ¬
    test_text = "æˆ‘æƒ³é¢„è®¢é…’åº—"
    
    # å•è¯·æ±‚æµ‹è¯•
    print("="*50)
    print("å•è¯·æ±‚æ€§èƒ½æµ‹è¯•")
    print("="*50)
    tester.single_request_test(test_text, num_requests=50)
    
    # å¹¶å‘æµ‹è¯•
    print("\n" + "="*50)
    print("å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print("="*50)
    test_texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3", "æµ‹è¯•æ–‡æœ¬4"]
    tester.concurrent_test(test_texts, num_threads=10)
è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼š

bash
python tests/performance_test.py
ğŸ“ ç¬¬ä¸ƒç« ï¼šé¡¹ç›®æ€»ç»“ä¸ä½¿ç”¨æŒ‡å—
7.1 å®Œæ•´ä½¿ç”¨æµç¨‹
bash
# æ­¥éª¤1ï¼šå…‹éš†é¡¹ç›®
git clone <é¡¹ç›®åœ°å€>
cd intent_recognition_project

# æ­¥éª¤2ï¼šé…ç½®ç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# æ­¥éª¤3ï¼šæ£€æŸ¥æ•°æ®
python -c "import pandas as pd; df=pd.read_csv('data/custom_dataset.csv'); print(f'æ•°æ®é‡: {len(df)}')"

# æ­¥éª¤4ï¼šå¯åŠ¨API
cd api
python app.py
# æˆ–
gunicorn -w 4 -b 0.0.0.0:5000 app:app

# æ­¥éª¤5ï¼šæµ‹è¯•API
# æ–°å¼€ç»ˆç«¯
python clients/python_client.py
# æˆ–
./clients/curl_examples.sh
7.2 å¸¸è§é—®é¢˜è§£å†³
é—®é¢˜1ï¼šç«¯å£è¢«å ç”¨
bash
# Linux/Mac
lsof -i :5000
kill -9 <PID>

# Windows
netstat -ano | findstr :5000
taskkill /PID <PID> /F
é—®é¢˜2ï¼šæ¨¡å‹åŠ è½½å¤±è´¥
python
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
import os
model_path = "models/best_bert_model"
print(f"æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {os.path.exists(model_path)}")
print(f"pytorch_model.binå­˜åœ¨: {os.path.exists(f'{model_path}/pytorch_model.bin')}")

# é‡æ–°ä¸‹è½½æ¨¡å‹
from transformers import BertTokenizer, BertForSequenceClassification
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=5)
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)
é—®é¢˜3ï¼šå†…å­˜ä¸è¶³
bash
# å‡å°æ‰¹å¤„ç†å¤§å°
# ä¿®æ”¹api/app.pyä¸­çš„max_lengthä¸º64
# æˆ–ä½¿ç”¨CPUæ¨¡å¼
device = torch.device('cpu')
7.3 æ‰©å±•å»ºè®®
æ·»åŠ è®¤è¯ï¼šä¸ºAPIæ·»åŠ API KeyéªŒè¯

æ—¥å¿—è®°å½•ï¼šæ·»åŠ è¯¦ç»†çš„è¯·æ±‚æ—¥å¿—

æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ï¼šæ”¯æŒå¤šç‰ˆæœ¬æ¨¡å‹

è‡ªåŠ¨æ‰©ç¼©å®¹ï¼šä½¿ç”¨Kubernetesç®¡ç†

ç›‘æ§å‘Šè­¦ï¼šé›†æˆPrometheus + Grafana