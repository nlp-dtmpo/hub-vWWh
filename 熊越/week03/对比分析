transformer分为4层，分别为输入层(前置embedding、positionEncoding)、输出层(前置embedding、positionEncoding)、编码器、解码器。
					其中编码器分为2块，其中有多头自注意力机制块和FeedForWord，并且他们中间链接是add&norm（残差和归一化）
					解码器分为3块，mask多头注意力块、多头自注意力块、FeedForWord组成，中间链接是add&norm（残差和归一化）
					多头自注意力实现为第一步：matul(q*kt)/(根号下词维度/多头数)，加add&norm。第二步：softmax 第三步：matul（第二步output,v）从而得到结果
bert:优点 1.基于transformer实现的编码器模型，比原生transformer多了一个切片。通过词嵌入和位置编码实现双向感知
     缺点 2.花费开销大，训练速度慢

prompt:优点 1.大模型底层都是使用transformer解码器推理生成 2.不需要微调 3.只需要设计提示词
	   缺点 1.反馈时间根据问题复杂程度决定，2.提示词不好界定 3.使用会花费成本
	   
regex：优点 1.通过正则表达式，能快速匹配出结果
	   缺点 1.只能通过表达式获取不能推理，错误率高

TFIDF：优点 1.TF是词频统计，使用时分词后会把每个词以0 1方式横向存储与内存中，当字重复则+=1， 公式：出现次数/文档总词数
			2.IDF逆向文档频率 结合TF变为统计分类很好的工具，例如当一个词高频在某个文档中出现而其他文档极少出现，则这个词越重要 log(总文件数目/(出现文件数+1))
			二者相乘能得到权重
	   缺点 1.不能准确理解上下文关系，只能做特征提取。2.无法解决长距离依赖 3.无序，无语法结构
